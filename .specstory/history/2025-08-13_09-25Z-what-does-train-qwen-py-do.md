<!-- Generated by SpecStory -->

# What does train_qwen.py do? (2025-08-13 09:25Z)

_**User**_

@train_qwen.py 이게 어떤 코드인지 알려줘.

---

_**Assistant**_

첨부된 `train_qwen.py` 파일을 분석해보겠습니다.


---

Read file: qwen-tuning/Qwen2.5-VL/qwen-vl-finetune/qwenvl/train/train_qwen.py

---

이 `train_qwen.py` 파일은 **Qwen2.5-VL 및 Qwen2-VL 멀티모달 모델의 파인튜닝을 위한 훈련 스크립트**입니다. 주요 특징들을 설명드리겠습니다:

## 🎯 **주요 기능**

### 1. **모델 지원**
- **Qwen2.5-VL**: 최신 버전의 멀티모달 모델
- **Qwen2-VL**: 이전 버전의 멀티모달 모델
- 이미지와 텍스트를 함께 처리할 수 있는 Vision-Language 모델

### 2. **선택적 파라미터 훈련**
```python
def set_model(model_args, model):
    # Vision Encoder, MLP, LLM 각각을 독립적으로 활성화/비활성화 가능
    - tune_mm_vision: Vision Encoder 파라미터 훈련 여부
    - tune_mm_mlp: MLP 레이어 파라미터 훈련 여부  
    - tune_mm_llm: LLM 파라미터 훈련 여부
```

### 3. **DeepSpeed 호환성**
- **분산 훈련** 지원
- **메모리 최적화**를 위한 DeepSpeed 엔진 사용
- `UnfreezeAfterDeepSpeed` 콜백으로 DeepSpeed 초기화 후 파라미터 재활성화

### 4. **커스텀 콜백**
- **`PrintLossCallback`**: 훈련 중 손실값과 예측 샘플 출력
- **`UnfreezeAfterDeepSpeed`**: DeepSpeed 환경에서 Vision Encoder 파라미터 재활성화

### 5. **데이터 처리**
- **일반 데이터 모듈**: `make_supervised_data_module`
- **패킹 데이터 모듈**: `make_supervised_data_module_packed` (메모리 효율성)
- 이미지 크기: 672x672 픽셀로 고정

## �� **주요 설정**

### 이미지 처리 설정
```python
min_pixels=451584, max_pixels=451584
resized_height=672, resized_width=672
```

### 모델 로딩 옵션
- `torch_dtype=torch.bfloat16`: 메모리 절약
- `low_cpu_mem_usage=False`: 분산 환경 호환성
- `trust_remote_code=True`: 커스텀 코드 허용

## �� **모니터링 기능**

1. **실시간 로그**: 손실값, 그래디언트 노름, 학습률, 처리 속도
2. **예측 샘플**: 100 스텝마다 실제 예측 결과 출력
3. **파라미터 상태**: 훈련 가능한 파라미터 개수 추적

## �� **사용 목적**

이 스크립트는 **멀티모달 AI 모델을 특정 태스크에 맞게 파인튜닝**하는 데 사용됩니다. 예를 들어:
- 이미지 캡셔닝
- 시각적 질의응답
- 이미지 기반 대화
- 특정 도메인의 이미지-텍스트 이해

특히 **부분적 파라미터 훈련**을 통해 메모리 효율성을 높이고, **DeepSpeed**를 통한 대규모 모델 훈련을 지원하는 것이 특징입니다.

---

_**User**_

@trainer.py 이거는 뭐하는 스크립트야?

---

_**Assistant**_

`trainer.py` 파일을 분석해보겠습니다. 이 파일은 **Qwen 멀티모달 모델의 훈련을 위한 커스텀 Trainer 확장** 스크립트입니다.

## 🎯 **주요 기능**

### 1. **Flash Attention 지원** (주석 처리됨)
```python
# def _flash_attention_forward(...):
#     """Flash Attention을 사용한 고속 어텐션 계산"""
#     # 메모리 효율적인 어텐션 구현
```
- 현재는 주석 처리되어 있지만, Flash Attention을 통한 메모리 최적화 기능 제공
- 대용량 모델 훈련 시 메모리 사용량 감소

### 2. **파라미터 훈련 상태 모니터링**
```python
def print_trainable_parameters_visual(self):
    """Vision 모듈의 훈련 가능한 파라미터 상태 출력"""
    # Vision Encoder의 각 블록별 훈련 상태 확인
    # Merger 모듈 훈련 상태 확인

def print_trainable_parameters(self):
    """LLM 모듈의 훈련 가능한 파라미터 상태 출력"""
    # 임베딩, 레이어, 정규화 레이어별 훈련 상태 확인
```

### 3. **고급 옵티마이저 설정**
```python
def create_optimizer(self):
    """다양한 학습률을 가진 파라미터 그룹 생성"""
```

#### **파라미터 그룹 분류:**
1. **일반 파라미터**: 기본 학습률 + weight decay
2. **Vision Tower 파라미터**: `vision_tower_lr` + weight decay
3. **MM Projector 파라미터**: `mm_projector_lr` + weight decay
4. **Bias 파라미터**: 기본 학습률 + weight decay 없음

### 4. **Monkey Patching**
```python
# 기존 클래스에 새로운 메서드 추가
Trainer.create_optimizer = create_optimizer
Qwen2VisionTransformerPretrainedModel.print_trainable_parameters = print_trainable_parameters_visual
Qwen2VLModel.print_trainable_parameters = print_trainable_parameters
```

## �� **핵심 특징**

### **다중 학습률 지원**
- **Vision Encoder**: `vision_tower_lr`로 별도 학습률 설정 가능
- **MM Projector (Merger)**: `mm_projector_lr`로 별도 학습률 설정 가능
- **LLM 파라미터**: 기본 학습률 사용

### **Weight Decay 분리**
- **LayerNorm/RMSNorm 파라미터**: weight decay 적용
- **Bias 파라미터**: weight decay 제외
- **일반 가중치**: weight decay 적용

### **훈련 상태 추적**
- Vision 모듈의 각 어텐션 블록별 훈련 상태 확인
- LLM 모듈의 각 레이어별 훈련 상태 확인
- Merger 모듈 훈련 상태 확인

## �� **사용 목적**

이 스크립트는 **멀티모달 모델의 효율적인 파인튜닝**을 위해 다음과 같은 기능을 제공합니다:

1. **메모리 최적화**: Flash Attention을 통한 메모리 사용량 감소
2. **유연한 학습률**: 모듈별로 다른 학습률 적용 가능
3. **상세한 모니터링**: 어떤 파라미터가 실제로 훈련되고 있는지 확인
4. **안정적인 훈련**: 적절한 weight decay 설정으로 과적합 방지

## 🔗 **train_qwen.py와의 관계**

- `train_qwen.py`에서 이 `trainer.py`의 기능들을 활용
- 특히 `UnfreezeAfterDeepSpeed` 콜백과 함께 사용하여 DeepSpeed 환경에서 안정적인 훈련 지원
- 멀티모달 모델의 복잡한 구조를 효율적으로 훈련할 수 있도록 도와주는 핵심 도구

이 스크립트는 **대규모 멀티모달 모델 훈련의 복잡성을 관리**하고 **효율적인 파인튜닝**을 가능하게 하는 중요한 역할을 합니다.

---

_**User**_

@data_qwen.py 이거는 뭐하는 코드야?

---

_**Assistant**_

`data_qwen.py` 파일을 분석해보겠습니다. 이 파일은 **Qwen 멀티모달 모델을 위한 데이터 처리 및 전처리 스크립트**입니다.

## 🎯 **주요 기능**

### 1. **멀티모달 데이터 처리**
```python
# 이미지와 비디오 모두 지원
DEFAULT_IMAGE_TOKEN = "<image>"
DEFAULT_VIDEO_TOKEN = "<video>"
IMAGE_TOKEN_INDEX = 151655
VIDEO_TOKEN_INDEX = 151656
```

### 2. **대화형 데이터 전처리**
```python
def preprocess_qwen_2_visual(sources, tokenizer, grid_thw_image, grid_thw_video):
    """대화형 데이터를 모델 입력 형식으로 변환"""
    # 시스템 메시지 추가
    # 이미지/비디오 토큰 처리
    # 채팅 템플릿 적용
```

### 3. **LazySupervisedDataset 클래스**
```python
class LazySupervisedDataset(Dataset):
    """지연 로딩 방식의 지도학습 데이터셋"""
```

#### **주요 특징:**
- **지연 로딩**: 메모리 효율성을 위해 필요할 때만 데이터 로드
- **멀티모달 지원**: 이미지, 비디오, 텍스트 동시 처리
- **에러 복구**: 파일 손상 시 다른 샘플로 대체
- **샘플링 지원**: 데이터셋별 샘플링 비율 설정 가능

### 4. **이미지 처리**
```python
def process_image_unified(self, image_file):
    """이미지 전처리 및 패치화"""
    # PIL로 이미지 로드
    # 이미지 프로세서로 전처리
    # 패치 크기 및 그리드 정보 반환
```

### 5. **비디오 처리**
```python
def process_video(self, video_file):
    """비디오 프레임 추출 및 처리"""
    # Decord를 사용한 비디오 로딩
    # 프레임 샘플링 (4-8프레임)
    # 시간 간격 기반 프레임 선택
```

### 6. **데이터 콜레이터**
```python
@dataclass
class DataCollatorForSupervisedDataset:
    """배치 생성을 위한 데이터 콜레이터"""
    # 시퀀스 패딩
    # 멀티모달 데이터 결합
    # 포지션 ID 생성
```

## 🔧 **핵심 처리 과정**

### **1. 데이터 로딩**
```python
# JSONL 또는 JSON 형식의 어노테이션 파일 읽기
# 샘플링 비율 적용
# 데이터 경로 설정
```

### **2. 멀티모달 토큰화**
```python
# <image> 토큰을 <|vision_start|> + 패드 + <|vision_end|>로 변환
# <video> 토큰을 비디오 전용 토큰으로 변환
# 채팅 템플릿 적용
```

### **3. 시각적 데이터 처리**
```python
# 이미지: 672x672 크기로 리사이즈, 패치화
# 비디오: 프레임 추출, 시간 정보 포함
# RoPE 인덱스 생성 (2D 위치 인코딩)
```

### **4. 배치 생성**
```python
# 시퀀스 길이 맞춤 (패딩)
# 멀티모달 데이터 결합
# 어텐션 마스크 생성
```

## 📊 **지원하는 데이터 형식**

### **입력 데이터 구조:**
```json
{
  "conversations": [
    {"from": "human", "value": "이 이미지를 설명해주세요 <image>"},
    {"from": "gpt", "value": "이 이미지는..."}
  ],
  "image": "path/to/image.jpg",
  "data_path": "/data/images/"
}
```

### **처리된 출력:**
```python
{
  "input_ids": tensor(...),      # 토큰화된 입력
  "labels": tensor(...),         # 학습 레이블
  "pixel_values": tensor(...),   # 이미지 패치
  "position_ids": tensor(...),   # 2D 위치 인코딩
  "attention_mask": tensor(...)  # 어텐션 마스크
}
```

## 🎯 **주요 설정 옵션**

### **이미지 설정:**
- `max_pixels`: 451584 (672x672)
- `min_pixels`: 451584
- 패치 크기: 14x14

### **비디오 설정:**
- `video_min_frames`: 4
- `video_max_frames`: 8
- `base_interval`: 4초 간격

### **데이터 처리:**
- `data_flatten`: 시퀀스 패킹 여부
- `sampling_rate`: 데이터셋별 샘플링 비율

## 🔗 **train_qwen.py와의 관계**

- `train_qwen.py`에서 `make_supervised_data_module()` 함수 호출
- 멀티모달 데이터를 모델이 이해할 수 있는 형식으로 변환
- 효율적인 배치 생성을 통해 훈련 속도 향상

이 스크립트는 **복잡한 멀티모달 데이터를 체계적으로 처리**하고 **대화형 AI 모델 훈련에 최적화된 데이터 파이프라인**을 제공하는 핵심 역할을 합니다.

---

