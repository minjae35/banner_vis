<!-- Generated by SpecStory -->

# Differences in output files for inference (2025-08-13 06:25Z)

_**User**_

@run_original_model_inference.py 여기에서는 output으로 이미지, output.txt, prompt.txt, results.json 파일들을 @original_model_inference_results/ 이렇게 묶어서 저장을 하는데, @run_inference.py 여기서는 왜 output로 뱉는 파일들이 달라? @run_inference.py 이것의 결과값들은 @inference_results/ 여기에 저장되어있어.

---

_**Assistant**_

두 스크립트의 결과 저장 방식이 다른 이유를 분석해보겠습니다.


---

Read file: experiments/inference/run_original_model_inference.py

---

<details>
                <summary>Searched codebase "save_results function inference_utils" • **14** results</summary>
            
| File | Lines |
|------|-------|
| `experiments/inference/inference_utils.py` | L156-188 |
| `논문/인퍼런스/inference/inference_utils.py` | L89-189 |
| `src/banner_visualized.py` | L5143-5500 |
| `src/banner_visualized.py` | L4264-4354 |
| `experiments/inference/run_inference.py` | L27-90 |
| `논문/인퍼런스/inference/run_inference.py` | L25-135 |
| `논문/인퍼런스/inference/run_classification_only.py` | L221-377 |
| `src/banner_visualized.py` | L4973-5087 |
| `experiments/inference/run_original_model_inference.py` | L48-186 |
| `논문/인퍼런스/inference/run_ocr_only.py` | L156-292 |
| `src/banner_visualized.py` | L4917-5023 |
| `논문/인퍼런스/inference/run_original_model_inference.py` | L92-184 |
| `experiments/inference/run_inference.py` | Lundefined-90 |
| `src/banner_visualized.py` | L5503-5553 |

</details>

---

Read file: experiments/inference/inference_utils.py

---

이제 두 스크립트의 차이점을 명확히 설명드리겠습니다.

## 두 스크립트의 결과 저장 방식 차이점

### 1. **`run_original_model_inference.py` (원본 모델 추론)**
- **저장 위치**: `./original_model_inference_results/`
- **저장 방식**: **개별 이미지별로 폴더 생성**
  ```
  original_model_inference_results/
  ├── 0643/
  │   ├── 0643.jpg          # 원본 이미지
  │   ├── result.json       # 추론 결과 (JSON)
  │   ├── output.txt        # 모델 출력 텍스트
  │   └── prompt.txt        # 사용된 프롬프트
  ├── 0648/
  │   ├── 0648.jpg
  │   ├── result.json
  │   ├── output.txt
  │   └── prompt.txt
  └── all_results.json      # 전체 결과 통합
  ```

### 2. **`run_inference.py` (체크포인트 추론)**
- **저장 위치**: `./inference_results/`
- **저장 방식**: **체크포인트별로 폴더 생성, 모든 이미지 결과를 하나의 파일에 통합**
  ```
  inference_results/
  ├── bal_equal/
  │   ├── bal_equal_standard_20250807_171206.json    # 모든 이미지 결과 통합
  │   └── bal_equal_standard_summary.txt             # 요약 정보
  ├── bal_equal_simple/
  │   ├── bal_equal_simple_standard_20250807_170327.json
  │   └── bal_equal_simple_standard_summary.txt
  └── ...
  ```

## 주요 차이점

### **저장 구조**
- **원본 모델**: 이미지별 개별 폴더 → 상세한 분석 가능
- **체크포인트**: 체크포인트별 통합 파일 → 대량 처리에 최적화

### **파일 종류**
- **원본 모델**: `result.json`, `output.txt`, `prompt.txt`, 원본 이미지
- **체크포인트**: `{checkpoint}_{prompt_type}_{timestamp}.json`, `summary.txt`

### **용도**
- **원본 모델**: 단일 모델의 상세한 결과 분석
- **체크포인트**: 여러 체크포인트의 성능 비교 및 대량 처리

### **코드 구현 차이**
```python
# run_original_model_inference.py (개별 저장)
out_dir = os.path.join(result_dir, img_name)
os.makedirs(out_dir, exist_ok=True)
# 각 이미지마다 개별 파일들 생성

# run_inference.py (통합 저장)
output_dir = os.path.join(RESULTS_BASE_DIR, checkpoint_name)
save_results(results, output_dir, checkpoint_name, prompt_type)
# 모든 이미지 결과를 하나의 JSON 파일로 통합
```

이러한 차이는 각 스크립트의 목적과 사용 시나리오에 맞춰 설계된 것입니다.

---

_**User**_

@run_inference.py 이것의 결과로 @bal_equal/ 여기에 @bal_equal_ocr_20250807_160805.json 이걸보면 "ocr_text"가 있는데 이게 무엇을 의미하는지 알려줘.

---

_**Assistant**_




---

Read file: experiments/inference/inference_results/bal_equal/bal_equal_ocr_20250807_160805.json

---

<details>
                <summary>Searched codebase "ocr_text OCR only inference" • **15** results</summary>
            
| File | Lines |
|------|-------|
| `논문/인퍼런스/inference/run_ocr_only.py` | L65-154 |
| `논문/인퍼런스/inference/run_classification_only.py` | L70-199 |
| `논문/인퍼런스/inference/run_ocr_only.py` | Lundefined-82 |
| `experiments/inference/inference_utils.py` | L86-156 |
| `논문/인퍼런스/inference/inference_utils.py` | L52-157 |
| `논문/인퍼런스/inference/run_ocr_only.py` | L82-208 |
| `논문/인퍼런스/inference/run_ocr_only.py` | L84-244 |
| `data/konana/konana_data.ipynb` | L923-968 |
| `논문/인퍼런스/inference/run_ocr_only.py` | L294-391 |
| `experiments/inference/inference_utils.py` | L86-182 |
| `논문/인퍼런스/inference/run_ocr_only.py` | L36-154 |
| `논문/banner_pipline.py` | L61-237 |
| `data/konana/banner_pipline.py` | L61-237 |
| `논문/인퍼런스/inference/inference_utils.py` | L87-183 |
| `논문/인퍼런스/inference/run_ocr_only.py` | L156-292 |

</details>

---

<details>
            <summary>Searched codebase "run_ocr_only.py" • **1** results</summary>
        
| File |
|------|
| `/home/intern2/banner_vis/논문/인퍼런스/inference/run_ocr_only.py` |

</details>

---

Read file: 논문/인퍼런스/inference/run_ocr_only.py

---

이제 `ocr_text`의 의미를 명확히 설명드리겠습니다.

## `ocr_text`의 의미

`bal_equal_ocr_20250807_160805.json` 파일의 `ocr_text`는 **현수막에서 추출된 텍스트 내용**을 의미합니다.

### **OCR (Optical Character Recognition) 전용 추론**

이 파일은 `run_ocr_only.py` 스크립트를 통해 생성된 것으로, 다음과 같은 특징이 있습니다:

### **1. OCR 전용 프롬프트 사용**
```python
OCR_PROMPT = (
    "<obj>이 현수막</obj>"
    "위 현수막에 적힌 모든 텍스트를 정확히 추출해주세요. "
    "OCR 과정에서 발생할 수 있는 오타(숫자, 한글)를 자동으로 수정하고, "
    "추출된 텍스트만 출력하세요."
)
```

### **2. 파일명 패턴**
- `bal_equal_ocr_20250807_160805.json`
- `{checkpoint_name}_ocr_{timestamp}.json`

### **3. 결과 구조**
```json
{
  "image_path": "/path/to/image.jpg",
  "image_name": "image.jpg", 
  "ocr_text": "현수막에서 추출된 실제 텍스트 내용",
  "timestamp": "2025-08-07T16:07:13.516065",
  "checkpoint_name": "bal_equal",
  "image_id": "image_id"
}
```

### **4. 실제 예시**
JSON 파일에서 보이는 `ocr_text` 예시들:

```json
// 아파트 광고
"ocr_text": "위치갑 송도 1공구아파트 및 단지내상가 포스코 자사고 앞 파격 조건 분양 032-870-6940"

// 공공 행사
"ocr_text": "제2회 유아체능단 영어발표회 성북구청 성북구도시관리공단"

// 상업 광고
"ocr_text": "1. Party Party 2. Happy Event 3. 종영markt, 크리스마스, 러브데이, 애티, 세일, 선생시, 아린니 4. 파티월드"
```

### **5. 목적**
- **텍스트 추출**: 현수막 이미지에서 텍스트만 정확히 추출
- **오타 수정**: OCR 과정에서 발생할 수 있는 오타 자동 수정
- **분류 준비**: 추출된 텍스트를 바탕으로 나중에 분류 작업 수행

### **6. 다른 추론과의 차이점**
- **일반 추론**: 이미지 + 텍스트 → 분류 결과
- **OCR 전용**: 이미지 → 텍스트 추출만
- **분류 전용**: 텍스트 → 분류 결과만

따라서 `ocr_text`는 현수막 이미지에서 실제로 읽어낸 텍스트 내용을 담고 있으며, 이는 현수막 분류 작업의 첫 번째 단계인 텍스트 인식 결과입니다.

---

_**User**_

이 코드를 자세히 분석해줘.

---

_**Assistant**_

이 `run_single_inference` 함수를 자세히 분석해드리겠습니다.

## 함수 개요
```python
def run_single_inference(model, processor, device, image_path, prompt_text):
```
- **목적**: 단일 이미지에 대해 VLM(Vision-Language Model) 추론을 수행
- **입력**: 모델, 프로세서, 디바이스, 이미지 경로, 프롬프트 텍스트
- **출력**: 추론 결과를 담은 딕셔너리 또는 None (실패 시)

## 단계별 분석

### 1. **초기 설정 및 검증**
```python
img_name = os.path.basename(image_path)

try:
    # 이미지 존재 확인
    if not os.path.exists(image_path):
        print(f"[ERROR] 이미지 파일이 존재하지 않음: {image_path}")
        return None
    
    # 프롬프트 텍스트 검증
    if not isinstance(prompt_text, str) or not prompt_text.strip():
        print(f"[ERROR] 유효하지 않은 프롬프트 텍스트: {prompt_text}")
        return None
```
- **이미지명 추출**: 파일 경로에서 파일명만 추출
- **이미지 존재 확인**: 파일이 실제로 존재하는지 검증
- **프롬프트 검증**: 프롬프트가 유효한 문자열인지 확인

### 2. **메시지 구성**
```python
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": f"file://{os.path.abspath(image_path)}"},
            {"type": "text", "text": prompt_text.strip()},
        ],
    }
]
```
- **멀티모달 메시지**: 이미지와 텍스트를 함께 포함
- **file:// 프로토콜**: 이미지 경로를 절대 경로로 변환하여 안전하게 처리
- **role**: "user"로 설정하여 사용자 입력임을 명시

### 3. **비전 정보 처리**
```python
image_inputs, video_inputs = process_vision_info(messages)

if not image_inputs:
    print(f"[ERROR] 이미지 입력이 없음: {image_path}")
    return None
```
- **이미지 로드**: PIL Image 객체로 변환
- **비디오 처리**: 현재는 사용하지 않음 (빈 리스트)
- **검증**: 이미지가 성공적으로 로드되었는지 확인

### 4. **모델 입력 준비**
```python
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

model_inputs = processor(
    text=[text],
    images=image_inputs,
    padding=True,
    return_tensors="pt",
).to(device)
```
- **채팅 템플릿 적용**: Qwen 모델의 채팅 형식으로 변환
- **토크나이징**: `tokenize=False`로 텍스트 그대로 반환
- **생성 프롬프트 추가**: `add_generation_prompt=True`로 모델이 응답할 준비
- **텐서 변환**: PyTorch 텐서로 변환하여 GPU로 이동

### 5. **추론 실행**
```python
with torch.no_grad():
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=256,
        do_sample=False,
        temperature=0.0,
        top_p=1.0,
    )
```
- **그래디언트 비활성화**: 메모리 효율성을 위해
- **생성 파라미터**:
  - `max_new_tokens=256`: 최대 256개 토큰 생성
  - `do_sample=False`: 확률적 샘플링 비활성화 (결정적)
  - `temperature=0.0`: 가장 확실한 토큰 선택
  - `top_p=1.0`: 모든 토큰 고려

### 6. **결과 처리**
```python
generated_ids_only = generated_ids[0][model_inputs["input_ids"].shape[1]:]
response_text = processor.batch_decode([generated_ids_only], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
```
- **생성된 토큰만 추출**: 입력 토큰을 제외하고 새로 생성된 토큰만 선택
- **디코딩**: 토큰을 텍스트로 변환
- **특수 토큰 제거**: `skip_special_tokens=True`로 불필요한 토큰 제거

### 7. **결과 반환**
```python
return {
    "image_path": image_path,
    "image_name": img_name,
    "prompt": prompt_text,
    "response": response_text,
    "timestamp": datetime.now().isoformat()
}
```
- **구조화된 결과**: 모든 관련 정보를 딕셔너리로 반환
- **메타데이터 포함**: 이미지 경로, 프롬프트, 타임스탬프 등

### 8. **에러 처리**
```python
except Exception as e:
    print(f"[ERROR] 추론 중 오류 발생 ({img_name}): {e}")
    return None
```
- **예외 처리**: 모든 예외를 포착하여 안전하게 처리
- **에러 로깅**: 어떤 이미지에서 오류가 발생했는지 명시
- **None 반환**: 실패 시 None을 반환하여 호출자가 처리할 수 있도록 함

## 주요 특징

### **안전성**
- 모든 단계에서 검증 수행
- 예외 상황에 대한 적절한 처리
- 파일 존재 여부 확인

### **효율성**
- `torch.no_grad()` 사용으로 메모리 절약
- 결정적 생성으로 일관된 결과

### **확장성**
- 다양한 프롬프트 타입 지원
- 구조화된 결과 반환으로 후처리 용이

이 함수는 현수막 분류 시스템의 핵심 추론 엔진으로, 안정적이고 효율적인 VLM 추론을 수행합니다.

---

